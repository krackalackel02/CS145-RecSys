{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "240ac74a",
      "metadata": {},
      "source": [
        "# Recommender Systems Analysis and Visualization\n",
        "This notebook performs an exploratory analysis of recommender systems using the Sim4Rec library.\n",
        "We'll generate synthetic data, compare multiple baseline recommenders, and visualize their performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c379bc2b",
      "metadata": {},
      "source": [
        "## Cell: Import libraries and set up environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98a397cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import shutil\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as sf\n",
        "from pyspark.sql import DataFrame, Window\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.types import DoubleType, ArrayType\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "\n",
        "# Fix for KeyError: 'HOME' on Windows\n",
        "if 'HOME' not in os.environ:\n",
        "    os.environ['HOME'] = os.path.expanduser(\"~\")\n",
        "if 'HADOOP_HOME' not in os.environ:\n",
        "    #need to be c://hadoop on Windows\n",
        "    os.environ['HADOOP_HOME'] = \"C:\\\\hadoop\"\n",
        "# Set both driver and worker to use the current Python executable (should be 3.11)\n",
        "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1df06693",
      "metadata": {},
      "source": [
        "## Initialize Spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63fe22c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "spark = (SparkSession.builder\n",
        "    .appName(\"RecSysVisualization\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.driver.memory\", \"4g\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"Spark session initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "283f6936",
      "metadata": {},
      "source": [
        "## Import competition modules "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdbbfd79",
      "metadata": {},
      "outputs": [],
      "source": [
        "from data_generator import CompetitionDataGenerator\n",
        "from simulator import CompetitionSimulator\n",
        "from sample_recommenders import (\n",
        "    RandomRecommender,\n",
        "    PopularityRecommender,\n",
        "    ContentBasedRecommender\n",
        ")\n",
        "from config import DEFAULT_CONFIG, EVALUATION_METRICS\n",
        "\n",
        "print(\"Competition modules imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d42ba0c",
      "metadata": {},
      "source": [
        "## Define custom recommender template\n",
        "Below is a template class for implementing a custom recommender system. You should extend this class with your own recommendation algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a713e0a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyRecommender:\n",
        "    \"\"\"\n",
        "    Template class for implementing a custom recommender.\n",
        "    \n",
        "    This class provides the basic structure required to implement a recommender\n",
        "    that can be used with the Sim4Rec simulator. Students should extend this class\n",
        "    with their own recommendation algorithm.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, seed=None):\n",
        "        \"\"\"\n",
        "        Initialize recommender.\n",
        "        \n",
        "        Args:\n",
        "            seed: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.seed = seed\n",
        "        # Add your initialization logic here\n",
        "    \n",
        "    def fit(self, log, user_features=None, item_features=None):\n",
        "        \"\"\"\n",
        "        Train the recommender model based on interaction history.\n",
        "        \n",
        "        Args:\n",
        "            log: Interaction log with user_idx, item_idx, and relevance columns\n",
        "            user_features: User features dataframe (optional)\n",
        "            item_features: Item features dataframe (optional)\n",
        "        \"\"\"\n",
        "        # Implement your training logic here\n",
        "        pass\n",
        "    \n",
        "    def predict(self, log, k, users, items, user_features=None, item_features=None, filter_seen_items=True):\n",
        "        \"\"\"\n",
        "        Generate recommendations for users.\n",
        "        \n",
        "        Args:\n",
        "            log: Interaction log with user_idx, item_idx, and relevance columns\n",
        "            k: Number of items to recommend\n",
        "            users: User dataframe\n",
        "            items: Item dataframe\n",
        "            user_features: User features dataframe (optional)\n",
        "            item_features: Item features dataframe (optional)\n",
        "            filter_seen_items: Whether to filter already seen items\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame: Recommendations with user_idx, item_idx, and relevance columns\n",
        "        \"\"\"\n",
        "        # Example of a random recommender implementation:\n",
        "        # Cross join users and items\n",
        "        recs = users.crossJoin(items)\n",
        "        \n",
        "        # Filter out already seen items if needed\n",
        "        if filter_seen_items and log is not None:\n",
        "            seen_items = log.select(\"user_idx\", \"item_idx\")\n",
        "            recs = recs.join(\n",
        "                seen_items,\n",
        "                on=[\"user_idx\", \"item_idx\"],\n",
        "                how=\"left_anti\"\n",
        "            )\n",
        "        \n",
        "        # Add random relevance scores\n",
        "        recs = recs.withColumn(\n",
        "            \"relevance\",\n",
        "            sf.rand(seed=self.seed)\n",
        "        )\n",
        "        \n",
        "        # Rank items by relevance for each user\n",
        "        window = Window.partitionBy(\"user_idx\").orderBy(sf.desc(\"relevance\"))\n",
        "        recs = recs.withColumn(\"rank\", sf.row_number().over(window))\n",
        "        \n",
        "        # Filter top-k recommendations\n",
        "        recs = recs.filter(sf.col(\"rank\") <= k).drop(\"rank\")\n",
        "        \n",
        "        return recs\n",
        "\n",
        "print(\"MyRecommender template defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526842aa",
      "metadata": {},
      "source": [
        "## Data Exploration Functions\n",
        "These functions help us understand the generated synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff67d39",
      "metadata": {},
      "outputs": [],
      "source": [
        "def explore_user_data(users_df):\n",
        "    \"\"\"\n",
        "    Explore user data distributions and characteristics.\n",
        "    \n",
        "    Args:\n",
        "        users_df: DataFrame containing user data\n",
        "    \"\"\"\n",
        "    print(\"=== User Data Exploration ===\")\n",
        "    \n",
        "    # Get basic statistics\n",
        "    print(f\"Total number of users: {users_df.count()}\")\n",
        "    \n",
        "    # User segments distribution\n",
        "    segment_counts = users_df.groupBy(\"segment\").count().toPandas()\n",
        "    print(\"\\nUser Segments Distribution:\")\n",
        "    for _, row in segment_counts.iterrows():\n",
        "        print(f\"  {row['segment']}: {row['count']} users ({row['count']/users_df.count()*100:.1f}%)\")\n",
        "    \n",
        "    # Plot user segments\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.pie(segment_counts['count'], labels=segment_counts['segment'], autopct='%1.1f%%', startangle=90, shadow=True)\n",
        "    plt.title('User Segments Distribution')\n",
        "    plt.axis('equal')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('user_segments_distribution.png')\n",
        "    plt.show()\n",
        "    print(\"User segments visualization saved to 'user_segments_distribution.png'\")\n",
        "    \n",
        "    # Convert to pandas for easier feature analysis\n",
        "    users_pd = users_df.toPandas()\n",
        "    \n",
        "    # Analyze user feature distributions\n",
        "    feature_cols = [col for col in users_pd.columns if col.startswith('user_attr_')]\n",
        "    if len(feature_cols) > 0:\n",
        "        # Take a sample of feature columns if there are many\n",
        "        sample_features = feature_cols[:min(5, len(feature_cols))]\n",
        "        \n",
        "        # Plot histograms for sample features\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        for i, feature in enumerate(sample_features):\n",
        "            plt.subplot(2, 3, i+1)\n",
        "            for segment in users_pd['segment'].unique():\n",
        "                segment_data = users_pd[users_pd['segment'] == segment]\n",
        "                plt.hist(segment_data[feature], alpha=0.5, bins=20, label=segment)\n",
        "            plt.title(f'Distribution of {feature}')\n",
        "            plt.xlabel('Value')\n",
        "            plt.ylabel('Count')\n",
        "            if i == 0:\n",
        "                plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('user_feature_distributions.png')\n",
        "        plt.show()\n",
        "        print(\"User feature distributions saved to 'user_feature_distributions.png'\")\n",
        "        \n",
        "        # Feature correlation heatmap\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        corr = users_pd[feature_cols].corr()\n",
        "        mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "        sns.heatmap(\n",
        "            corr, mask=mask, cmap='coolwarm', vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, annot=False, fmt='.2f'\n",
        "        )\n",
        "        plt.title('User Feature Correlations')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('user_feature_correlations.png')\n",
        "        plt.show()\n",
        "        print(\"User feature correlations saved to 'user_feature_correlations.png'\")\n",
        "\n",
        "def explore_item_data(items_df):\n",
        "    \"\"\"\n",
        "    Explore item data distributions and characteristics.\n",
        "    \n",
        "    Args:\n",
        "        items_df: DataFrame containing item data\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Item Data Exploration ===\")\n",
        "    \n",
        "    # Get basic statistics\n",
        "    print(f\"Total number of items: {items_df.count()}\")\n",
        "    \n",
        "    # Item categories distribution\n",
        "    category_counts = items_df.groupBy(\"category\").count().toPandas()\n",
        "    print(\"\\nItem Categories Distribution:\")\n",
        "    for _, row in category_counts.iterrows():\n",
        "        print(f\"  {row['category']}: {row['count']} items ({row['count']/items_df.count()*100:.1f}%)\")\n",
        "    \n",
        "    # Plot item categories\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.pie(category_counts['count'], labels=category_counts['category'], autopct='%1.1f%%', startangle=90, shadow=True)\n",
        "    plt.title('Item Categories Distribution')\n",
        "    plt.axis('equal')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('item_categories_distribution.png')\n",
        "    plt.show()\n",
        "    print(\"Item categories visualization saved to 'item_categories_distribution.png'\")\n",
        "    \n",
        "    # Convert to pandas for easier feature analysis\n",
        "    items_pd = items_df.toPandas()\n",
        "    \n",
        "    # Analyze price distribution\n",
        "    if 'price' in items_pd.columns:\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        \n",
        "        # Overall price distribution\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.hist(items_pd['price'], bins=30, alpha=0.7)\n",
        "        plt.title('Overall Price Distribution')\n",
        "        plt.xlabel('Price')\n",
        "        plt.ylabel('Count')\n",
        "        \n",
        "        # Price by category\n",
        "        plt.subplot(1, 2, 2)\n",
        "        for category in items_pd['category'].unique():\n",
        "            category_data = items_pd[items_pd['category'] == category]\n",
        "            plt.hist(category_data['price'], alpha=0.5, bins=20, label=category)\n",
        "        plt.title('Price Distribution by Category')\n",
        "        plt.xlabel('Price')\n",
        "        plt.ylabel('Count')\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('item_price_distributions.png')\n",
        "        plt.show()\n",
        "        print(\"Item price distributions saved to 'item_price_distributions.png'\")\n",
        "    \n",
        "    # Analyze item feature distributions\n",
        "    feature_cols = [col for col in items_pd.columns if col.startswith('item_attr_')]\n",
        "    if len(feature_cols) > 0:\n",
        "        # Take a sample of feature columns if there are many\n",
        "        sample_features = feature_cols[:min(5, len(feature_cols))]\n",
        "        \n",
        "        # Plot histograms for sample features\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        for i, feature in enumerate(sample_features):\n",
        "            plt.subplot(2, 3, i+1)\n",
        "            for category in items_pd['category'].unique():\n",
        "                category_data = items_pd[items_pd['category'] == category]\n",
        "                plt.hist(category_data[feature], alpha=0.5, bins=20, label=category)\n",
        "            plt.title(f'Distribution of {feature}')\n",
        "            plt.xlabel('Value')\n",
        "            plt.ylabel('Count')\n",
        "            if i == 0:\n",
        "                plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('item_feature_distributions.png')\n",
        "        plt.show()\n",
        "        print(\"Item feature distributions saved to 'item_feature_distributions.png'\")\n",
        "\n",
        "def explore_interactions(history_df, users_df, items_df):\n",
        "    \"\"\"\n",
        "    Explore interaction patterns between users and items.\n",
        "    \n",
        "    Args:\n",
        "        history_df: DataFrame containing interaction history\n",
        "        users_df: DataFrame containing user data\n",
        "        items_df: DataFrame containing item data\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Interaction Data Exploration ===\")\n",
        "    \n",
        "    # Get basic statistics\n",
        "    total_interactions = history_df.count()\n",
        "    total_users = users_df.count()\n",
        "    total_items = items_df.count()\n",
        "    \n",
        "    print(f\"Total interactions: {total_interactions}\")\n",
        "    print(f\"Interaction density: {total_interactions / (total_users * total_items) * 100:.4f}%\")\n",
        "    \n",
        "    # Users with interactions\n",
        "    users_with_interactions = history_df.select(\"user_idx\").distinct().count()\n",
        "    print(f\"Users with at least one interaction: {users_with_interactions} ({users_with_interactions/total_users*100:.1f}%)\")\n",
        "    \n",
        "    # Items with interactions\n",
        "    items_with_interactions = history_df.select(\"item_idx\").distinct().count()\n",
        "    print(f\"Items with at least one interaction: {items_with_interactions} ({items_with_interactions/total_items*100:.1f}%)\")\n",
        "    \n",
        "    # Distribution of interactions per user\n",
        "    interactions_per_user = history_df.groupBy(\"user_idx\").count().toPandas()\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(interactions_per_user['count'], bins=20)\n",
        "    plt.title('Distribution of Interactions per User')\n",
        "    plt.xlabel('Number of Interactions')\n",
        "    plt.ylabel('Number of Users')\n",
        "    \n",
        "    # Distribution of interactions per item\n",
        "    interactions_per_item = history_df.groupBy(\"item_idx\").count().toPandas()\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(interactions_per_item['count'], bins=20)\n",
        "    plt.title('Distribution of Interactions per Item')\n",
        "    plt.xlabel('Number of Interactions')\n",
        "    plt.ylabel('Number of Items')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('interaction_distributions.png')\n",
        "    plt.show()\n",
        "    print(\"Interaction distributions saved to 'interaction_distributions.png'\")\n",
        "    \n",
        "    # Analyze relevance distribution\n",
        "    if 'relevance' in history_df.columns:\n",
        "        relevance_dist = history_df.groupBy(\"relevance\").count().toPandas()\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(relevance_dist['relevance'].astype(str), relevance_dist['count'])\n",
        "        plt.title('Distribution of Relevance Scores')\n",
        "        plt.xlabel('Relevance Score')\n",
        "        plt.ylabel('Count')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('relevance_distribution.png')\n",
        "        plt.show()\n",
        "        print(\"Relevance distribution saved to 'relevance_distribution.png'\")\n",
        "    \n",
        "    # If we have user segments and item categories, analyze cross-interactions\n",
        "    if 'segment' in users_df.columns and 'category' in items_df.columns:\n",
        "        # Join with user segments and item categories\n",
        "        interaction_analysis = (history_df\n",
        "            .join(users_df.select('user_idx', 'segment'), on='user_idx')\n",
        "            .join(items_df.select('item_idx', 'category'), on='item_idx')\n",
        "        )\n",
        "        \n",
        "        # Count interactions by segment and category\n",
        "        segment_category_counts = interaction_analysis.groupBy('segment', 'category').count().toPandas()\n",
        "        \n",
        "        # Create a pivot table\n",
        "        pivot_table = segment_category_counts.pivot(index='segment', columns='category', values='count').fillna(0)\n",
        "        \n",
        "        # Plot heatmap\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(pivot_table, annot=True, fmt='g', cmap='viridis')\n",
        "        plt.title('Interactions Between User Segments and Item Categories')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('segment_category_interactions.png')\n",
        "        plt.show()\n",
        "        print(\"Segment-category interactions saved to 'segment_category_interactions.png'\")\n",
        "\n",
        "print(\"Data exploration functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8df14c9",
      "metadata": {},
      "source": [
        "## Performance Visualization Functions\n",
        "These functions create visualizations for comparing recommender performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca97ffbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_recommender_performance(results_df, recommender_names):\n",
        "    \"\"\"\n",
        "    Visualize the performance of recommenders in terms of revenue and key metrics.\n",
        "    \n",
        "    Args:\n",
        "        results_df: DataFrame with evaluation results\n",
        "        recommender_names: List of recommender names\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(16, 16))\n",
        "    \n",
        "    # Plot total revenue comparison\n",
        "    plt.subplot(3, 2, 1)\n",
        "    x = np.arange(len(recommender_names))\n",
        "    width = 0.35\n",
        "    plt.bar(x - width/2, results_df['train_total_revenue'], width, label='Training')\n",
        "    plt.bar(x + width/2, results_df['test_total_revenue'], width, label='Testing')\n",
        "    plt.xlabel('Recommender')\n",
        "    plt.ylabel('Total Revenue')\n",
        "    plt.title('Total Revenue Comparison')\n",
        "    plt.xticks(x, results_df['name'])\n",
        "    plt.legend()\n",
        "    \n",
        "    # Plot average revenue per iteration\n",
        "    plt.subplot(3, 2, 2)\n",
        "    plt.bar(x - width/2, results_df['train_avg_revenue'], width, label='Training')\n",
        "    plt.bar(x + width/2, results_df['test_avg_revenue'], width, label='Testing')\n",
        "    plt.xlabel('Recommender')\n",
        "    plt.ylabel('Avg Revenue per Iteration')\n",
        "    plt.title('Average Revenue Comparison')\n",
        "    plt.xticks(x, results_df['name'])\n",
        "    plt.legend()\n",
        "    \n",
        "    # Plot discounted revenue comparison (if available)\n",
        "    plt.subplot(3, 2, 3)\n",
        "    if 'train_discounted_revenue' in results_df.columns and 'test_discounted_revenue' in results_df.columns:\n",
        "        plt.bar(x - width/2, results_df['train_discounted_revenue'], width, label='Training')\n",
        "        plt.bar(x + width/2, results_df['test_discounted_revenue'], width, label='Testing')\n",
        "        plt.xlabel('Recommender')\n",
        "        plt.ylabel('Avg Discounted Revenue')\n",
        "        plt.title('Discounted Revenue Comparison')\n",
        "        plt.xticks(x, results_df['name'])\n",
        "        plt.legend()\n",
        "    \n",
        "    # Plot revenue trajectories\n",
        "    plt.subplot(3, 2, 4)\n",
        "    markers = ['o', 's', 'D', '^']\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    \n",
        "    for i, name in enumerate(results_df['name']):\n",
        "        # Combined train and test trajectories\n",
        "        train_revenue = results_df.iloc[i]['train_revenue']\n",
        "        test_revenue = results_df.iloc[i]['test_revenue']\n",
        "        \n",
        "        # Ensure they are lists\n",
        "        if isinstance(train_revenue, (float, np.floating, int, np.integer)):\n",
        "            train_revenue = [train_revenue]\n",
        "        if isinstance(test_revenue, (float, np.floating, int, np.integer)):\n",
        "            test_revenue = [test_revenue]\n",
        "        \n",
        "        iterations = list(range(len(train_revenue))) + list(range(len(test_revenue)))\n",
        "        revenues = train_revenue + test_revenue\n",
        "        \n",
        "        plt.plot(iterations, revenues, marker=markers[i % len(markers)], \n",
        "                 color=colors[i % len(colors)], label=name)\n",
        "        \n",
        "        # Add a vertical line to separate train and test\n",
        "        if i == 0:  # Only add the line once\n",
        "            plt.axvline(x=len(train_revenue)-0.5, color='k', linestyle='--', alpha=0.3, label='Train/Test Split')\n",
        "    \n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Revenue')\n",
        "    plt.title('Revenue Trajectory (Training → Testing)')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Plot ranking metrics comparison - Training\n",
        "    plt.subplot(3, 2, 5)\n",
        "    \n",
        "    # Select metrics to include\n",
        "    ranking_metrics = ['precision_at_k', 'ndcg_at_k', 'mrr', 'hit_rate']\n",
        "    ranking_metrics = [m for m in ranking_metrics if f'train_{m}' in results_df.columns]\n",
        "    \n",
        "    # Create bar groups\n",
        "    bar_positions = np.arange(len(ranking_metrics))\n",
        "    bar_width = 0.8 / len(results_df)\n",
        "    \n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']  # Extended palette\n",
        "    \n",
        "    for i, (_, row) in enumerate(results_df.iterrows()):\n",
        "        model_name = row['name']\n",
        "        offsets = (i - len(results_df)/2 + 0.5) * bar_width\n",
        "        metric_values = [row[f'train_{m}'] for m in ranking_metrics]\n",
        "        plt.bar(bar_positions + offsets, metric_values, bar_width, label=model_name, \n",
        "                color=colors[i % len(colors)], alpha=0.7)\n",
        "    \n",
        "    plt.xlabel('Metric')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Ranking Metrics Comparison (Training Phase)')\n",
        "    plt.xticks(bar_positions, [m.replace('_', ' ').title() for m in ranking_metrics])\n",
        "    plt.legend()\n",
        "    \n",
        "    # Plot ranking metrics comparison - Testing\n",
        "    plt.subplot(3, 2, 6)\n",
        "    \n",
        "    ranking_metrics = ['precision_at_k', 'ndcg_at_k', 'mrr', 'hit_rate']\n",
        "    ranking_metrics = [m for m in ranking_metrics if f'test_{m}' in results_df.columns]\n",
        "    \n",
        "    # Get best-performing model (by test total revenue) to highlight\n",
        "    best_model_idx = results_df['test_total_revenue'].idxmax()\n",
        "    best_model_name = results_df.iloc[best_model_idx]['name']\n",
        "    \n",
        "    bar_positions = np.arange(len(ranking_metrics))\n",
        "    bar_width = 0.8 / len(results_df)\n",
        "    \n",
        "    for i, (_, row) in enumerate(results_df.iterrows()):\n",
        "        model_name = row['name']\n",
        "        offsets = (i - len(results_df)/2 + 0.5) * bar_width\n",
        "        metric_values = [row[f'test_{m}'] for m in ranking_metrics]\n",
        "        plt.bar(bar_positions + offsets, metric_values, bar_width, label=model_name, \n",
        "                color=colors[i % len(colors)],\n",
        "                alpha=0.7 if model_name != best_model_name else 1.0)\n",
        "    \n",
        "    plt.xlabel('Metric')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Ranking Metrics Comparison (Test Phase)')\n",
        "    plt.xticks(bar_positions, [m.replace('_', ' ').title() for m in ranking_metrics])\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('recommender_performance_comparison.png')\n",
        "    plt.show()\n",
        "    print(\"\\nPerformance visualizations saved to 'recommender_performance_comparison.png'\")\n",
        "\n",
        "def visualize_detailed_metrics(results_df, recommender_names):\n",
        "    \"\"\"\n",
        "    Create detailed visualizations for each metric and recommender.\n",
        "    \n",
        "    Args:\n",
        "        results_df: DataFrame with evaluation results\n",
        "        recommender_names: List of recommender names\n",
        "    \"\"\"\n",
        "    # Create a figure for metric trajectories\n",
        "    plt.figure(figsize=(16, 16))\n",
        "    \n",
        "    # Get all available metrics\n",
        "    all_metrics = []\n",
        "    if len(results_df) > 0 and 'train_metrics' in results_df.columns:\n",
        "        first_train_metrics = results_df.iloc[0]['train_metrics'][0]\n",
        "        all_metrics = list(first_train_metrics.keys())\n",
        "    \n",
        "    # Select key metrics to visualize\n",
        "    key_metrics = ['revenue', 'discounted_revenue', 'precision_at_k', 'ndcg_at_k', 'mrr', 'hit_rate']\n",
        "    key_metrics = [m for m in key_metrics if m in all_metrics]\n",
        "    \n",
        "    # Plot metric trajectories for each key metric\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    markers = ['o', 's', 'D', '^']\n",
        "    \n",
        "    for i, metric in enumerate(key_metrics):\n",
        "        if i < 6:  # Limit to 6 metrics to avoid overcrowding\n",
        "            plt.subplot(3, 2, i+1)\n",
        "            \n",
        "            for j, name in enumerate(results_df['name']):\n",
        "                row = results_df[results_df['name'] == name].iloc[0]\n",
        "                \n",
        "                # Get metric values for training phase\n",
        "                train_values = []\n",
        "                for train_metric in row['train_metrics']:\n",
        "                    if metric in train_metric:\n",
        "                        train_values.append(train_metric[metric])\n",
        "                \n",
        "                # Get metric values for testing phase\n",
        "                test_values = []\n",
        "                for test_metric in row['test_metrics']:\n",
        "                    if metric in test_metric:\n",
        "                        test_values.append(test_metric[metric])\n",
        "                \n",
        "                # Plot training phase\n",
        "                plt.plot(\n",
        "                    range(len(train_values)),\n",
        "                    train_values,\n",
        "                    marker=markers[j % len(markers)],\n",
        "                    color=colors[j % len(colors)],\n",
        "                    linestyle='-',\n",
        "                    label=f\"{name} (train)\"\n",
        "                )\n",
        "                \n",
        "                # Plot testing phase\n",
        "                plt.plot(\n",
        "                    range(len(train_values), len(train_values) + len(test_values)),\n",
        "                    test_values,\n",
        "                    marker=markers[j % len(markers)],\n",
        "                    color=colors[j % len(colors)],\n",
        "                    linestyle='--',\n",
        "                    label=f\"{name} (test)\"\n",
        "                )\n",
        "                \n",
        "                # Add a vertical line to separate train and test\n",
        "                if j == 0:  # Only add the line once\n",
        "                    plt.axvline(x=len(train_values) - 0.5, color='k', linestyle='--', alpha=0.3, label='Train/Test Split')\n",
        "            \n",
        "            # Get metric info from EVALUATION_METRICS if available\n",
        "            if metric in EVALUATION_METRICS:\n",
        "                metric_info = EVALUATION_METRICS[metric]\n",
        "                metric_name = metric_info['name']\n",
        "                plt.title(f\"{metric_name} Trajectory\")\n",
        "            else:\n",
        "                plt.title(f\"{metric.replace('_', ' ').title()} Trajectory\")\n",
        "            \n",
        "            plt.xlabel('Iteration')\n",
        "            plt.ylabel('Value')\n",
        "            \n",
        "            # Add legend to the last plot or the final one in layout\n",
        "            if i == len(key_metrics) - 1 or i == 5:\n",
        "                plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('recommender_metrics_trajectories.png')\n",
        "    plt.show()\n",
        "    print(\"Detailed metrics visualizations saved to 'recommender_metrics_trajectories.png'\")\n",
        "    \n",
        "    # Create a correlation heatmap of metrics\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    \n",
        "    # Extract metrics columns\n",
        "    metric_cols = [col for col in results_df.columns if col.startswith('train_') or col.startswith('test_')]\n",
        "    metric_cols = [col for col in metric_cols if not col.endswith('_metrics') and not col.endswith('_revenue')]\n",
        "    \n",
        "    if len(metric_cols) > 1:\n",
        "        correlation_df = results_df[metric_cols].corr()\n",
        "        # Plot heatmap\n",
        "        sns.heatmap(correlation_df, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "        plt.title('Correlation Between Metrics')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('metrics_correlation_heatmap.png')\n",
        "        plt.show()\n",
        "        print(\"Metrics correlation heatmap saved to 'metrics_correlation_heatmap.png'\")\n",
        "\n",
        "def calculate_discounted_cumulative_gain(recommendations, k=5, discount_factor=0.85):\n",
        "    \"\"\"\n",
        "    Calculate the Discounted Cumulative Gain for recommendations.\n",
        "    \n",
        "    Args:\n",
        "        recommendations: DataFrame with recommendations (must have relevance column)\n",
        "        k: Number of items to consider\n",
        "        discount_factor: Factor to discount gains by position\n",
        "    \n",
        "    Returns:\n",
        "        float: Average DCG across all users\n",
        "    \"\"\"\n",
        "    # Group by user and calculate per-user DCG\n",
        "    user_dcg = []\n",
        "    grouped_recs = (recommendations\n",
        "        .groupBy(\"user_idx\")\n",
        "        .agg(sf.collect_list(sf.struct(\"relevance\", \"rank\")).alias(\"recommendations\"))\n",
        "    ).collect()\n",
        "    \n",
        "    for row in grouped_recs:\n",
        "        user_id = row['user_idx']\n",
        "        user_rec_list = sorted(row['recommendations'], key=lambda x: x[1])\n",
        "        \n",
        "        dcg = 0\n",
        "        for i, rec in enumerate(user_rec_list[:k]):\n",
        "            rel = rec['relevance']\n",
        "            # Apply discount based on position\n",
        "            dcg += rel * (discount_factor ** i)\n",
        "        user_dcg.append(dcg)\n",
        "    \n",
        "    return np.mean(user_dcg) if user_dcg else 0.0\n",
        "\n",
        "print(\"Performance visualization functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dafd511d",
      "metadata": {},
      "source": [
        "## Generate Data and Perform Exploratory Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "755a414d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# We'll create a smaller dataset for demonstration.\n",
        "config = DEFAULT_CONFIG.copy()\n",
        "config['data_generation']['n_users'] = 1000  # Reduced from 10,000\n",
        "config['data_generation']['n_items'] = 200   # Reduced from 1,000\n",
        "config['data_generation']['seed'] = 42       # Fixed seed for reproducibility\n",
        "\n",
        "# Get train-test split parameters\n",
        "train_iterations = config['simulation']['train_iterations']\n",
        "test_iterations = config['simulation']['test_iterations']\n",
        "\n",
        "print(f\"Running train-test simulation with {train_iterations} training iterations and {test_iterations} testing iterations\")\n",
        "\n",
        "# Initialize data generator\n",
        "data_generator = CompetitionDataGenerator(\n",
        "    spark_session=spark,\n",
        "    **config['data_generation']\n",
        ")\n",
        "\n",
        "# Generate user data\n",
        "users_df = data_generator.generate_users()\n",
        "print(f\"Generated {users_df.count()} users\")\n",
        "\n",
        "# Generate item data\n",
        "items_df = data_generator.generate_items()\n",
        "print(f\"Generated {items_df.count()} items\")\n",
        "\n",
        "# Generate initial interaction history\n",
        "history_df = data_generator.generate_initial_history(\n",
        "    config['data_generation']['initial_history_density']\n",
        ")\n",
        "print(f\"Generated {history_df.count()} initial interactions\")\n",
        "\n",
        "print(\"\\n=== Starting Exploratory Data Analysis ===\")\n",
        "explore_user_data(users_df)\n",
        "explore_item_data(items_df)\n",
        "explore_interactions(history_df, users_df, items_df)\n",
        "\n",
        "print(\"Exploratory analysis complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e906765a",
      "metadata": {},
      "source": [
        "## Set up Generators and Recommenders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc4322f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data generators for the simulator\n",
        "user_generator, item_generator = data_generator.setup_data_generators()\n",
        "\n",
        "# Initialize the recommenders we want to compare\n",
        "recommenders = [\n",
        "    RandomRecommender(seed=42),\n",
        "    PopularityRecommender(alpha=1.0, seed=42),\n",
        "    ContentBasedRecommender(similarity_threshold=0.0, seed=42),\n",
        "    MyRecommender(seed=42)  # Custom template class\n",
        "]\n",
        "recommender_names = [\"Random\", \"Popularity\", \"ContentBased\", \"MyRecommender\"]\n",
        "\n",
        "# Fit each recommender on the initial history\n",
        "for recommender in recommenders:\n",
        "    recommender.fit(log=data_generator.history_df)\n",
        "\n",
        "print(\"Recommenders set up and initial fit complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7134f68a",
      "metadata": {},
      "source": [
        "## Train-Test Simulation and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4376fdfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, recommender in zip(recommender_names, recommenders):\n",
        "    print(f\"\\nEvaluating {name}:\")\n",
        "    \n",
        "    # Clean up any existing simulator data directory for this recommender\n",
        "    simulator_data_dir = f\"simulator_train_test_data_{name}\"\n",
        "    if os.path.exists(simulator_data_dir):\n",
        "        shutil.rmtree(simulator_data_dir)\n",
        "        print(f\"Removed existing simulator data directory: {simulator_data_dir}\")\n",
        "    \n",
        "    # Ensure 'relevance' column is LongType for compatibility\n",
        "    from pyspark.sql.types import LongType\n",
        "\n",
        "    history_df_casted = data_generator.history_df.withColumn(\n",
        "        \"relevance\", data_generator.history_df[\"relevance\"].cast(LongType())\n",
        "    )\n",
        "\n",
        "    # Initialize simulator\n",
        "    simulator = CompetitionSimulator(\n",
        "        user_generator=user_generator,\n",
        "        item_generator=item_generator,\n",
        "        data_dir=simulator_data_dir,\n",
        "        log_df=history_df_casted,  # Use casted DataFrame\n",
        "        conversion_noise_mean=config['simulation']['conversion_noise_mean'],\n",
        "        conversion_noise_std=config['simulation']['conversion_noise_std'],\n",
        "        spark_session=spark,\n",
        "        seed=config['data_generation']['seed']\n",
        "    )\n",
        "    \n",
        "    # Run simulation with train-test split\n",
        "    train_metrics, test_metrics, train_revenue, test_revenue = simulator.train_test_split(\n",
        "        recommender=recommender,\n",
        "        train_iterations=train_iterations,\n",
        "        test_iterations=test_iterations,\n",
        "        user_frac=config['simulation']['user_fraction'],\n",
        "        k=config['simulation']['k'],\n",
        "        filter_seen_items=config['simulation']['filter_seen_items'],\n",
        "        retrain=config['simulation']['retrain']\n",
        "    )\n",
        "    \n",
        "    # Calculate average metrics\n",
        "    train_avg_metrics = {}\n",
        "    for metric_name in train_metrics[0].keys():\n",
        "        values = [m[metric_name] for m in train_metrics]\n",
        "        train_avg_metrics[f\"train_{metric_name}\"] = np.mean(values)\n",
        "    \n",
        "    test_avg_metrics = {}\n",
        "    for metric_name in test_metrics[0].keys():\n",
        "        values = [m[metric_name] for m in test_metrics]\n",
        "        test_avg_metrics[f\"test_{metric_name}\"] = np.mean(values)\n",
        "    \n",
        "    # Store results\n",
        "    results.append({\n",
        "        \"name\": name,\n",
        "        \"train_total_revenue\": sum(train_revenue),\n",
        "        \"test_total_revenue\": sum(test_revenue),\n",
        "        \"train_avg_revenue\": np.mean(train_revenue),\n",
        "        \"test_avg_revenue\": np.mean(test_revenue),\n",
        "        \"train_metrics\": train_metrics,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"train_revenue\": train_revenue,\n",
        "        \"test_revenue\": test_revenue,\n",
        "        **train_avg_metrics,\n",
        "        **test_avg_metrics\n",
        "    })\n",
        "    \n",
        "    # Print summary for this recommender\n",
        "    print(f\"  Training Phase - Total Revenue: {sum(train_revenue):.2f}\")\n",
        "    print(f\"  Testing Phase - Total Revenue: {sum(test_revenue):.2f}\")\n",
        "    performance_change = ((sum(test_revenue) / len(test_revenue)) / (sum(train_revenue) / len(train_revenue)) - 1) * 100\n",
        "    print(f\"  Performance Change: {performance_change:.2f}%\")\n",
        "\n",
        "print(\"\\n=== Simulation and evaluation complete for all recommenders. ===\")\n",
        "\n",
        "# Convert to DataFrame for easy comparison\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(\"test_total_revenue\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Print summary table\n",
        "print(\"\\nRecommender Evaluation Results (sorted by test revenue):\")\n",
        "summary_cols = [\n",
        "    \"name\", \"train_total_revenue\", \"test_total_revenue\", \n",
        "    \"train_avg_revenue\", \"test_avg_revenue\",\n",
        "    \"train_precision_at_k\", \"test_precision_at_k\",\n",
        "    \"train_ndcg_at_k\", \"test_ndcg_at_k\",\n",
        "    \"train_mrr\", \"test_mrr\",\n",
        "    \"train_discounted_revenue\", \"test_discounted_revenue\"\n",
        "]\n",
        "summary_cols = [col for col in summary_cols if col in results_df.columns]\n",
        "\n",
        "if len(summary_cols) > 0:\n",
        "    print(results_df[summary_cols].to_string(index=False))\n",
        "else:\n",
        "    print(\"No summary columns to display.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2fb279",
      "metadata": {},
      "source": [
        "## Results Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf8b729",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Visualizing Recommender Performance ===\")\n",
        "visualize_recommender_performance(results_df, recommender_names)\n",
        "visualize_detailed_metrics(results_df, recommender_names)\n",
        "\n",
        "print(\"\\nVisualization complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23747224",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
